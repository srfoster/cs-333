0:03 It's for sure recording this little red button. 0:09 I took a nap and got over myself. So, let's do this, I guess. 0:15 Uh, simplest to most complex. Got datmss, which is like a thing, and then tupils, 0:24 which sounds like it would be two, but it can be more than two. Usually I think 0:30 of tupils as like uh points on like uh cartisian coordinate or like 0:38 3D space. Then we got arrays and then we got lists. Arrays and lists are very 0:46 similar in in on paper they're very similar but arrays are actually 0:53 specifically in uh a chunk of memory so 1:00 they're easier to access a lot of things in uh big of time. 1:06 And then we got trees and 1:13 those are usually have like one node and then some number of leaves uh like 1:19 binary search trees and al trees and B trees and then we got 1:26 I guess B trees is the next one. So there uh B trees having more than one 1:36 item in the node and then uh graphs. 1:44 Yeah, I think that covers all graphs have uh connections that are outside of 1:51 like a single link. So you can have multiple 1:57 things linked to multiple things. And I think that's sort of the fundamental difference there. 2:03 And then uh complexity wise, yeah. Oh yeah, this one I was answering 2:11 wrong every single time. So big O of one means constant time. So you you plug in 2:18 something and you get something out and it's like one step. And then you got big O of N. So 2:25 uh that would be like traversing an array which is part of the one of the next questions. 2:31 So you got to look through each thing. And then we got uh log of n which when 2:36 we have you know trees with the links then each layer is gonna 2:44 uh the number of layers is going to be well I should say the number of layers 2:50 in a wellbalanced tree is going to be log of the number of layers and then 2:56 that also applies even if it's like three things per node or you know 3:02 whatever And then n login is when we have 3:07 something like a binary search tree that you know we have the different layers but we still have to uh traverse each 3:16 one and then do an operation on top of that and then 3:22 and then we have all like the n squared uh polomial time which we'll talk about 3:29 more like p and np and And then big O 2 to the n is the exponential time. 3:39 Then this guy returns the sum of numbers from 1 to n. 3:45 So we can do that iteratively where we have like you know while whatever is 3:52 less than n depends on how we want to set it up or for loop we like I equals 3:58 zero and count up to those numbers uh 4:05 that's the n the bigo of n case but then we 4:11 also have Gaus's method So that's the n * quantity n + 1 / two. 4:21 We can do that. Write an algorithm searches for target 4:26 value in unsorted array. So this discussion is about looking for 4:32 a number in an array and kind of talking about how uh 4:38 big O is just N because we assume like we're going to have to go through each 4:43 number in that array or each value in that array. But best case uh so big 4:49 omega best case it could be the first number and then we'd stop. We would have that early exit. And then big theta 4:55 would be like the average case which is usually going to be the number of 5:01 elements divided by two. And then 5:09 yeah, that was kind of what we're talking about with the linear search. If we're looking for three, we're going to 5:14 do like four n number of elements in the array. We'll compare like is this three? 5:22 No. You know, increment I is this three? No. Increment I is this three? Yes. And 5:27 then you can exit early. 5:33 Finds the minimum value in an unsorted array. So this one uh all of them are n 5:41 because you have to look at every single element to make sure that 5:47 one of the remaining elements is not less than the one that you're testing with. So then you have like a current 5:53 minimum you're like is this value less than the current minimum? No. then you 5:59 just increment I and you go to the next one. Or if it is, then you do the variable swap and then you increment to 6:05 the next one. Write an algorithm that performs two 6:11 tasks. Checks if the array contains duplicate values, returns total count elements. So this is basically the same 6:18 idea. There's like uh we'd have to go through the array. uh 6:27 I think it'd be n^ squ and we take each value and be like okay 6:33 this is the value we're checking you know uh the first value like are there 6:38 any other threes check the next one next one next one next one next one nope okay let's check you know uh fours next one 6:44 next one next one and this discussion is talking more about like 6:50 uh that operation of checking every single value in the array is going to be 6:57 massively more uh computationally 7:02 uh expensive than returning the total number of elements in the array because at the 7:08 very end like I mean you could keep track of that all along the way and have a few uh writes 7:17 you know readrites comparisons and then you would already have this at the end. So the 7:24 big O of N is relatively negligible compared to big O of N squ. 7:36 Uh this is almost the exact same question. So this is going to be n squ again and 7:42 initialize the matrix. Uh same discussion about dominant terms, right? 7:48 is going to have, you know, n squared versus search the row for a specific value just be n being 7:57 uh cuz that's the length of that row anyway. Does this did I actually answer this 8:02 question? Okay. Yeah. So, dominant terms 8:08 discussion and then dominant terms discussion. 8:14 Okay. Assertion sort. So discuss the big picture significance of insertion sort. 8:22 So insertion sort is the one that is 8:28 in place and 8:33 yeah insertion sort and selection sort. 8:39 So, insertion sort is going to have uh 8:47 it checks for the lowest one, puts it in place, and then looks for the lowest 8:52 one, puts it in place, looks for the lowest one, puts it in place. And so, it's going to have a constant 9:00 number of operations, uh, because it has to check every single element to see if it's the lowest. So, 9:06 it's going to do that every single time. So it's very consistent and it uh does 9:14 not move a lot of things around. So it's sorted in place. 9:19 So yeah, this is the discussion provide example of inputs showing the best case 9:25 already sorted. So and worst case reverse sorted. It's still going to look 9:31 at every single one. So, it doesn't know if there's something lower than one. I 9:38 mean, unless you tell it ahead of time, let's say something lower than 10 in a set of 100 numbers that are all, you 9:44 know, greater than 10, it still has to check every single one and then move them back and forth 9:50 as opposed to uh selection sort, which uh does the comparisons to see like, 9:57 okay, is there anything lower than this to the left? and then it like moves 10:02 moves moves moves. So in average case it's going to be significantly faster 10:08 because there's usually at least a few numbers that are already in order. So 10:14 then it can you know do less operations there. I think that's what you were looking for. 10:23 C recursive and iterative examples of factorial function. So 10:28 recursive Is this Oh, it can draw. 10:36 So factorial is the n 10:41 times the n minus one. 10:49 So when we 10:54 when we do that recursively, we can actually take that uh that value n * factorial of n 11:02 minus one and it will go all the way down until the base case of one and then 11:08 you know it's one or iterative where it you know 11:14 increments uh you know four uh 11:21 you know n numbers it'll be like one and then okay we'll get that sum well 11:26 product and then two get that product three get that product for the number of 11:32 n so the two different ways of going about it and 11:39 the discussion which I don't necessarily like but I guess it's technically accurate is a lot of The 11:48 recursive versions of programming are straight from math. 11:55 Um whereas iterative ones require a little bit more programmatic type 12:00 thinking three implementations. So naive 12:06 recursive uh this is where we're not keeping track of anything where when you have I I 12:14 always look at these as like triangles where every step you have to do the 12:20 steps before it and then every like as you go all the way down uh you have to 12:28 recomputee all of those. Whereas if you memoize it, then as you go up and you've 12:35 already looked at one, you know, the Fibonacci of one, two, blah blah blah blah, then it's not going to have to 12:41 recalculate every single step of that all the way down that triangle. And then iterative bottom up. 12:50 That's I mean it's pretty obvious just like you 12:56 you have your your your three variables where you got your n 13:01 uh n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n minus one, n minus 2 and then you add them up and then you keep like 13:07 swapping out those variables for each iteration of n. 13:12 And uh why this is fundamental, it's cuz the naive recursive is like the 13:20 classic example of like stack overflow type stuff. Is it stack? 13:27 Yeah, it would be stack usually depending on the language. I suppose you could push it under the heap and then 13:34 memoized uh like it's a trick that can avoid 13:40 stack overflow. make it significantly faster. But then you're looking at other write operations and trying to balance 13:47 uh your floating point operations versus your 13:53 right operations and then iterate bottom up uh 13:59 just keeps the values. So it's a lot more successive readrs but the space and 14:07 time that it's taking up is relatively small. Compare and contrast tree structures 14:14 binary search tree versus AVL trees. So why self balance trees were to 14:20 develop and their significance in computer science. So if a binary search tree is completely one-sided, it just 14:26 becomes a link list. And uh that means that we're taking a list 14:35 or a tree that should have log n levels 14:40 and have login search features and turning it into 14:46 you know straight link list 14:51 versus AVL trees. And in this case, we're having a trade-off between having 14:57 to perform extra operations every time something's added to the tree versus 15:03 having less operations every time the tree is searched. Which, you know, if 15:08 you're creating a tree because you're trying to search it, then that's your dominant use. But if you're just 15:15 organizing something and you're not doing a whole lot of searches, then there may be, you know, there may be 15:22 other uh algorithms you want to use there. 15:30 Explain B trees. So 15:36 if uh I I discussed this already. So the 15:42 height the depth of a tree is going to be determined it's it's always going to 15:47 be log of the number of edges on that node. So if you have two then it's 15:55 always going to be log two. If you have three then it's going to be log three. So depending on 16:02 what sort of data you're working with, you need to be able to make that decision like okay, do we need to be 16:08 able to uh look up different things and will these nodes fit into memory and you 16:16 know you got to constantly be adjusting your data algorithm methods to match 16:25 what your actual goals are. Compare data structures for storing graphs. 16:31 Uh discuss big picture significance of graph representation and why it's 16:37 important to consider various graph data structure representations carefully. This is just an extension of the 16:42 previous question. So if uh graphs are usually used for 16:48 some sort of like relationships or waiting of edges. Um so when we talk 16:54 about stuff like page rank, right? If we're indexing uh billions of web pages, then you know 17:03 we're not gonna you don't necessarily want a graph that has 17:08 you know one node with all of these edges. So, uh you can have a graph of 17:15 matrices or a matrix of graphs and um 17:20 depending on uh whether you need something to fit 17:27 into RAM uh for the trace operations, then you might have a smaller graph of 17:34 larger uh data chunks, you know, that are indexed. Or if you need something 17:40 that's faster and you have uh the space to fit everything in RAM then or memory 17:49 cache memory is a better word for that then 17:54 you know maybe you want to use a larger or a smaller 18:01 graph of larger data objects. Depends. And in most cases, you're going 18:07 to be constantly having to re-evaluate that. 18:14 Implement breadth first search and depth first search. So, 18:22 uh, depth first, obviously, it's looking, uh, that's more like a, uh, 18:28 shortest path sort of thing, like, okay, I need to find a path from the top of 18:33 something to the bottom of something or left to right or whatever. Then that depth first is going to try and make the 18:39 most progress as fast as possible. But there's cases where uh you know you 18:45 could end up in loops where the breadth first search is going to help avoid those sorts of loops and stuff like 18:51 that. So depending on how the data is organized if you know you have no loops 18:56 like if you've already adjusted your graph to make sure that there's no loops then DFS can be way better but like 19:05 intermittently you'll still have to do BFS searches to make sure that there's no loops and so on and so forth. 19:13 Uh this this is yeah 19:18 BFS. So Dyster's algorithm is significant how it extends BFS to handle weighted 19:25 graphs. Um this one is 19:31 one of many I have found out uh shortest paths type algorithms. uh which would be 19:37 like for any waitings in a graph, it's going to 19:44 uh look at all of the different edges and have like a running total and then if it finds some other running total 19:51 that's shorter than that, it can check the other path. And uh these sort of 19:58 weighted graph traversal algorithms, these shortest path algorithms are going 20:04 to be specialized for different things. Uh I think Dystra's was one of the original shortest path 20:11 algorithms for some networking stuff. I don't remember the full history but yeah 20:20 uh implement minimum spanning tree algorithms. So, Prims is uh more or less 20:28 going to follow a similar path to Dyster's algorithm uh where it's going to start at a point and it's going to 20:33 look at the you know one next node, one next node, one next node. It might need 20:39 to backtrack if there's a dead end. And it's going to traverse the entire tree 20:47 and eventually come up with the entire minimum spanning tree. 20:52 And depending on how the data is arranged, that might be great. And then there's crucall which picks random 21:02 nodes around and then figures out how those nodes are 21:09 connected to each other. So it's like mold growing on a petri dish. So I think 21:16 one of the things I was looking at was actually saying like crucals can be used to simulate mold on petri dishes whereas 21:25 prim can be used to simulate uh uh 21:34 mushroom growth. What's the mcelium growth starting from when they or root growth? So it's kind of fun. 21:45 implement greedy algorithms. So greedy algorithms are when you can take one 21:50 brute force solution and apply it to something and it will stay consistently 21:56 uh accurate and useful. So with like the uh activity selection in that case it's 22:03 saying uh whichever activity ends first is always going to be uh or if you 22:10 always pick the activity that ends first it's always going to give you the best uh activity selection 22:17 which is great when greedy algorithms exist. And this also kind of goes to like the PNP discussion about like 22:25 reducing things and like coming up with like the best algorithms and finding the greedy algorithm that works for a 22:32 solution, you know, is always great, but they don't necessarily exist 22:37 uh or they're not necessarily easy to find and applying the same sort of 22:42 methodology to all problems is not useful. Uh same thing with fractional knapsack where you if you can take 22:50 chunks of things uh in this case the highest value divided by the weight then 22:56 that's always going to give you the most value for the weight that you can fit in the map. 23:03 But as we're about to see in a few questions that doesn't always work. 23:11 Merge sort as a divide and conquer algorithm. So I'm going to talk about merge sort 23:19 and quick sort together. So merge sort is going to be very 23:28 consistent, very predictable for the most part and it's going to continually 23:34 divide each thing uh divide the set into chunks 23:42 and then until that last chunk is only one element and then it will start comparing those chunks and it'll create 23:49 like okay now I have uh elements well, pairs of elements and then it'll go 23:55 through and have like quads of elements or five, sometimes they're quad ones. 24:01 And um sometimes 24:07 merge sort can be quicker than quick sort because if quick sort is completely 24:14 uh out of order then and or it uh picks 24:19 bad pivot points then uh quicksort is 24:24 going to have to look it's going to have to do a ton more comparisons 24:30 and merge sort. So depending on what you're doing with 24:35 your data, one may be better than the other. So like if you have completely unsorted data that you know is going to 24:41 be an absolute show, uh merge sort might be the best solution for like the 24:47 initial sort, but then if you're doing periodic sorts on already partially sorted data, then uh quicksort is 24:54 usually going to be quicker. 25:00 Why quicksord is faster than merge sword despite worst case complexity? Yes, I 25:07 believe I answered that. Implement dynamic program programming 25:12 solution for the 01 knapsack problem. So, this is what we previously discussed 25:18 where if you need to take the entirety of an item and put it in the knapsack 25:24 that you're not necessarily going to be able to use the greedy method to come up 25:30 with the best solution in which case um you you one of the ways is the DP table 25:38 that it talks about uh which is a dynamic programming table where you can 25:46 track a matrix of those solutions and then uh basically use a maximum flow 25:55 problem which I think is coming up uh to determine what that actual best path is 26:02 to get the the biggest number but that path is going to change based off of 26:07 every previous decision. So you that will help you detect like dead ends uh 26:14 that will get a lower total than what would have otherwise been possible. 26:24 Longest common subsequence problem. So discuss significance of LCS 26:29 bionirmatics version control and other applications. So these are 26:36 uh more or less pattern recognition. So if we're looking 26:44 for like in biioinformatics um 26:53 words so we're looking to compare chunks of 27:00 these data sets to to find the this sort of pattern recognition to be able to say 27:07 like uh you know these are the differences and this parts are common. So in like 27:14 version control and the diffs uh it will you know compare those common 27:21 subsequences and then it'll split out what's actually different and then it'll look for more common subsequences. 27:27 So that way it knows okay we don't need to change all of the stuff like we don't 27:33 need to completely rewrite a file. we can just rewrite the parts that are different 27:39 and then um same thing with like biioiniratics. If we're looking at genomes or 27:47 fingerprints and we can find uh these matched patterns in there. 27:55 uh then we can say okay if this longest common subsequence is greater than this 28:02 threshold then there's a you know one in bazillion chance that this fingerprint 28:08 doesn't belong to this person which is greater than the number of people in the world 28:13 so yeah I think that answers that 28:21 uh this is what I was talking about before fulkerson maximum flow for our room pro 28:27 algorithm. So this is very similar to the uh graph traversal stuff in that 28:35 um when you have these weighted edges 28:41 uh you're always going to be looking for like which paths add up to the greatest 28:46 amount of flow. And then 28:52 oh yeah the discussion about max flow min cut. So in a lot of cases when we're 28:57 looking at this if like the max flow equals min cut then we can find bottlenecks and things. Um so like if we 29:05 have this big network that's connected by this single 29:10 pipe but you know edge then uh you have this other huge network. Then when we 29:17 take that minimum cut and we cut the one edge and it has you know value of x then 29:24 we always know that the max flow is going to be x but that is the bottleneck and then you can start shifting things 29:30 around and this can help uh 29:35 help us organize things better for one but it can also help us with the 29:40 dynamic programming type stuff when we're trying to find um the largest 29:47 value paths and technically you could reverse it and find the lowest value paths 29:56 and SAT 3AT to show the threes is complete. Discuss 30:01 the significance PNP complete in light of your definitions. Discuss SAT 3AT two 30:07 set. Discuss why reductions are fundamental to complexity theory. So 30:14 uh SAT 3 sat 2 SAT or satisiability and 30:21 it's talking about having uh boolean logic 30:27 and uh being able to find which 30:33 being able to find the problems to the solutions is kind of the idea of why 30:39 those are significant because if we can take a problem, input something, get an 30:45 output, then like we can know that. But if we're trying to do that backwards, uh 30:51 being able to break that problem up into Boolean logic type stuff, um for 30:58 instance, factoring is the one that always gets brought up, like factoring massive numbers and finding prime 31:04 factors and stuff like that. If we can find the satisfiability 31:11 set for those large problems then it makes them trivial to solve stuff like 31:18 factoring which is also how we do encryption. So part of that discussion is uh the 31:27 reductions. So if we have like single uh boolean chunks like usually we can 31:34 handle those uh terms are they called terms I think they're 31:40 called terms and so if we have uh single term things and we like a or a or a and 31:47 that will bring us up to three. uh if we have two terms like a or b then we can 31:53 uh like add that like a or b or b so we can 32:00 always inflate that and then uh if we're taking 32:06 some chunks we can also reduce those to three sat which can then be reduced to 32:12 two sat which can be reduced to one set where uh you know we take chaining where 32:17 it's like um A or B or C or D then we 32:23 can take uh like A or B and then substitute what B or C and create that 32:30 length. So like A or B or X and then have X or C or D and then we can have 32:37 that into three set. So the idea of that reducability um taking these satisfy satisfiability 32:45 problems and continually reducing them to something that we can solve in polomial time would allow us to well 32:54 solve everything. Uh which talks about P and NP. Um so P are problems that we 33:02 know we can solve and we can solve them in polomial time. uh NP means we can 33:11 get we can check an answer to a problem in polomial time. Uh, and then NP 33:18 complete being um, well, let me back up. 33:25 P and NP uh, being P is an P is a subset of NP. And the 33:34 question really is like whether or not they are uh, 33:41 the same set. And that's where the whole discussion comes. But NP being able to 33:47 like check the solution to a problem uh for things like uh sudoku was the one 33:53 that came up or protein folding like we can 33:58 uh we can solve a sudoku in whatever amount of time. 34:05 It's not polinomial time, it's exponential time. Cuz as those things get bigger and bigger, the amount of 34:11 computation to solve them explodes exponentially. But if we have a solution, we can check it in polomial 34:18 time and be like, yes, that is a solution. 34:26 Yeah. 34:33 talk about that. I think that's everything.